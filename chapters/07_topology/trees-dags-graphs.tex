\section{Distributed Systems are Trees on Top of DAGs on Top of Graphs}
\marginnote{From ./AE-Specifications-ETH/standalone/Trees-DAGs-Graphs.tex}

This essay explores the layered graph-theoretic nature of distributed systems. At the lowest layer, physical and logical interconnects form undirected \textbf{graphs}. On top of this lie \textbf{DAGs} representing dependency, scheduling, and locking relationships. At the top, application-level consistency and authority are imposed via \textbf{trees} such as namespace hierarchies, leadership structures, and commit chains. We further examine how modern datacenters, populated by diverse xPUs (CPUs, GPUs, IPUs, DPUs), break the illusion of shared memory and necessitate protocol designs that exploit the native graph structure using mechanisms such as RDMA.

\subsection{Graphs: The Physical and Logical Fabric}
The physical topology of a datacenter is a graph: nodes represent compute units (CPUs, GPUs, IPUs, etc.) and edges represent communication links (Ethernet, NVLink, InfiniBand, etc.). These links may have diverse properties:
\begin{itemize}[noitemsep]
  \item Bandwidth and latency asymmetries
  \item Failures or congestion under load
  \item Scheduled or dynamic routing paths
\end{itemize}

Unlike the shared memory abstraction, these links form a non-uniform, fault-prone, and inherently asynchronous substrate. Real computation in modern datacenters occurs \textit{on this graph}---not above it.

\subsection{DAGs: Causality and Locking}
On top of the physical graph lies a directed acyclic graph (DAG) representing \textbf{causality, scheduling, and consistency constraints}. DAGs arise in:
\begin{itemize}[noitemsep]
  \item \textbf{Transaction dependencies}: Operations must follow a directed order to preserve causality.
  \item \textbf{Lock hierarchies}: Preventing deadlock requires acquiring locks in a fixed topological order.
  \item \textbf{Build systems and job schedulers}: Tasks must respect dependencies.
\end{itemize}

\subsection*{Locking as a DAG}
Databases employ lock hierarchies structured as DAGs to prevent circular waits. For example, the following might form a hierarchy:
\begin{enumerate}[label=\arabic*.]
  \item Lock table
  \item Then row
  \item Then field
\end{enumerate}
Each level narrows scope and follows a partial order. Enforcing that locks are acquired in topological order avoids cycles and hence deadlock.

\subsection{Trees: Names, Commit Chains, and Leaders}
At the top of the stack are trees. These structures are usually logical:
\begin{itemize}[noitemsep]
  \item \textbf{Namespace hierarchies}: e.g., file systems, DNS.
  \item \textbf{Leadership trees}: elected leaders per region, rack, or quorum.
  \item \textbf{Consensus and commits}: commit chains or logs form trees (or more precisely, forests with fork resolution).
\end{itemize}

These trees impose structure on the otherwise messy DAGs and graphs below, enabling:
\begin{itemize}[noitemsep]
  \item Easier authority delegation
  \item Fault domain containment
  \item Clear lineage and rollback support
\end{itemize}

\subsection{Breaking the Shared Memory Illusion}
Shared memory simplifies programming but breaks down in distributed xPU environments:
\begin{itemize}[noitemsep]
  \item Memory isnâ€™t uniformly addressable
  \item Coherence protocols are expensive or infeasible
  \item Latency variance introduces uncertainty in synchronization
\end{itemize}

\subsection*{RDMA: Network as Memory Bus}
Remote Direct Memory Access (RDMA) partially restores shared memory semantics:
\begin{itemize}[noitemsep]
  \item Allows direct writes/reads between NICs with low latency
  \item Bypasses kernel and CPU involvement
  \item Supports zero-copy semantics for performance
\end{itemize}

But RDMA also forces a shift:
\begin{itemize}[noitemsep]
  \item You must think \textbf{asynchronously}
  \item Buffers must be explicitly registered and tracked
  \item Failures are explicit, not hidden
\end{itemize}

\subsection{Exploiting the Graph: The Path Forward}
To fully exploit xPU networks:
\begin{itemize}
  \item Treat communication paths as first-class citizens
  \item Build coordination mechanisms that reflect graph topology
  \item Favor protocols that can adapt dynamically to congestion and partitioning
\end{itemize}

New system designs should:
\begin{enumerate}
  \item Replace locking with message-passing wherever feasible
  \item Encode application semantics in DAGs, not linear logs
  \item Use explicit versioning and conflict resolution mechanisms
\end{enumerate}

\subsection{Conclusion}
Distributed systems are not built on the abstraction of shared memory. They are constructed on a layered composition:
\begin{description}[style=unboxed,leftmargin=0cm]
  \item[Graphs:] physical connectivity
  \item[DAGs:] causal and logical dependencies
  \item[Trees:] naming, consensus, and leadership
\end{description}

The challenge of distributed systems is to harmonize these layers while respecting the physical realities of the system. To do so, we must leave behind illusions of synchrony and embrace graph-native programming models.