\documentclass[../../../OAE-SPEC-MAIN.tex]{subfiles}
\begin{document}

\section{Infiniband vs Ethernet}
\marginnote{From ./AE-Specifications-ETH/Infiniband.tex}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 \subsection{High-Level Overview}
At a high level, \textbf{Ethernet} and \textbf{InfiniBand} both provide
packet-switched networking, yet they originate from very different
design philosophies and ecosystems.  The widespread perception that
\textbf{InfiniBand is more reliable} arises from the architectural and
operational differences examined below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Origins and Design Philosophy}
\begin{itemize}
  \item \textbf{Ethernet} evolved as a best-effort LAN.  Reliability,
        ordering, and latency guarantees were relegated to higher
        layers (TCP/IP), Quality of Service, or RDMA over Converged
        Ethernet (RoCE).
  \item \textbf{InfiniBand} was conceived for high-performance
        computing (HPC) and modern data-center fabrics, prioritising
        low latency, high throughput, lossless delivery, and
        \emph{built-in} reliability.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Transport and Reliability Model}
\begin{itemize}
  \item \textbf{Ethernet (traditional)} is best-effort; packets may be
        dropped, duplicated, or reordered.  Higher layers must restore
        reliability.
  \item \textbf{InfiniBand} embeds a reliable transport protocol in
        hardware, handling acknowledgments, retransmissions, and flow
        control with minimal host-CPU involvement.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Congestion and Flow Control}
\begin{itemize}
  \item \textbf{Ethernet}: early 802.3 had none.  Data-Center Bridging
        (DCB) and Priority Flow Control (PFC) improve matters but add
        complexity and are not always enabled end-to-end.
  \item \textbf{InfiniBand}: credit-based flow control and end-to-end
        congestion management prevent buffer overruns without dropping
        traffic.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Packet-Loss Behaviour}
\begin{itemize}
  \item \textbf{Ethernet}: under load or misconfiguration, packets drop
        and TCP must recover, causing latency spikes.
  \item \textbf{InfiniBand}: packets are rarely dropped; congestion
        produces back-pressure instead.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Latency and Jitter}
\begin{itemize}
  \item \textbf{Ethernet}: latency and jitter depend on traffic,
        buffering, and TCP recovery.
  \item \textbf{InfiniBand}: microsecond-scale latency and very low
        jitter via lightweight stack, hardware offload, and zero-copy
        RDMA.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RDMA Support}
\begin{itemize}
  \item \textbf{Ethernet}: RoCE provides RDMA but demands a tuned,
        lossless fabric (PFC, ECN, buffer sizing), often fragile and
        vendor-specific.
  \item \textbf{InfiniBand}: native RDMA with hardware reliability and
        no special tuning.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CPU Overhead}
\begin{itemize}
  \item \textbf{Ethernet}: full TCP/IP stack consumes CPU unless
        offloaded by SmartNICs.
  \item \textbf{InfiniBand}: host-channel adapters offload
        packetisation, ordering, and reliability, yielding lower CPU
        utilisation.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ecosystem and Deployment}
\begin{itemize}
  \item \textbf{Ethernet}: ubiquitous, inexpensive, multi-vendor, and
        scaling to 800\,Gb/s.
  \item \textbf{InfiniBand}: dominant in latency-sensitive HPC/AI
        but niche, costlier, and largely single-vendor (NVIDIA/Mellanox).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Summary Table}
\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Feature} & \textbf{Ethernet} & \textbf{InfiniBand}\\
\midrule
Reliability        & Best-effort (TCP)         & Hardware-enforced\\
Latency            & Milli-second typical      & Micro-second\\
Jitter             & High                     & Very low\\
Congestion control & Optional (DCB/PFC)       & Built-in\\
Packet loss        & Possible                 & Avoided by design\\
RDMA               & RoCE (complex)           & Native\\
CPU overhead       & High (software stack)    & Low (offload)\\
Primary use        & General networking       & HPC / AI clusters\\
\bottomrule
\end{tabular}
\caption{High-level comparison of Ethernet and InfiniBand.}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RoCE: RDMA over Converged Ethernet}
\subsection{Variants}
\begin{itemize}
  \item \textbf{RoCE v1}: Layer 2 only, not routable.
  \item \textbf{RoCE v2}: UDP/Layer 3, IP-routable.
\end{itemize}

\subsection{Why Reliable Deployment is Hard}
\begin{enumerate} 
  \item \textbf{Lossless Fabric Requirement}: depends on PFC; mis-tunes
        cause deadlocks and head-of-line blocking.
  \item \textbf{UDP Transport}: inherits best-effort IP semantics unless
        the fabric is tightly managed.
  \item \textbf{Ecosystem Coordination}: NICs, drivers, libraries
        (\texttt{libibverbs}), and applications must align or fall back
        to TCP.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Why Ethernet Remains Dominant}
\begin{itemize}
  \item \textbf{Cost \& Ubiquity}: every datacenter already runs
        Ethernet; hardware is commoditized.
  \item \textbf{Interoperability}: multi-vendor openness avoids lock-in.
  \item \textbf{Performance Road-map}: speeds have risen from
        10\,Gb/s to 800\,Gb/s.
  \item \textbf{Software Ecosystem}: the global Internet stack assumes
        Ethernet/TCP.
  \item \textbf{RoCE as Bridge}: hyperscalers deploy RoCE successfully
        by exerting strict control over their fabrics.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conclusion}
InfiniBand remains the gold standard for ultra-low-latency, highly
reliable HPC workloads.  Ethernet is closing the gap through RoCE,
SmartNIC offloads, DCB/PFC, and ever-faster links.  Its dominance stems
from universality and cost, not intrinsic technical superiority.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Design Goals for a Next-Generation Fabric}
\begin{enumerate}
  \item Sub-microsecond latency with deterministic throughput.
  \item Hardware-enforced reliability (acknowledgment \& retransmission
        in silicon).
  \item \emph{RDMA-first} semantics: zero-copy \texttt{PUT}, \texttt{GET},
        and atomic operations.
  \item Programmability: P4/eBPF pipelines in NICs and switches.
  \item Security by design: cryptographic authN/authZ and fine-grained
        access control.
  \item Clock-agnostic operation: causal or reversible timing models.
  \item Composable transports: reliable/unreliable, ordered/unordered
        as required.
  \item Multi-tenant virtual fabrics on shared hardware.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Key Building Blocks}
\begin{itemize}
  \item \textbf{SmartNICs}: onboard CPUs or FPGAs for protocol state,
        reversibility, and EPI/ONT registers.
  \item \textbf{Flow-Aware Non-Switch Fabric}: dynamic,
        congestion-aware path scheduling.
  \item \textbf{Unified Declarative Transport}: intent-based API
        replacing TCP/IP or InfiniBand verbs.
  \item \textbf{Fabric-Wide Memory Space}: global RDMA address space
        with capability-based security.
\end{itemize}


\subsection{Design Framework}


\begin{description}
	\item [Something Old] Knowledge == captured information
	\item [Something New] RED == Information surprisal (the answer to a yes/no question)
	\item [Something Borrowed] == Tie-in to Rust model (for RPC - Alice owns but Bob borrows)
	\item [Something Blue] Semantics (Meaning) -- The SmartNIC/IPU understands the context
	\item [Something Green]  OCP Green for Open Syntax -- goes over PCIe. 
\end{description}



 
 
\marginnote{From ./AE-Specifications-ETH/Infiniband.tex}
At a high level, Ethernet and InfiniBand serve similar roles as network technologies, but they come from very different design philosophies and ecosystems. The perception (and often the reality) that InfiniBand is more reliable than Ethernet stems from multiple architectural and operational differences:

\begin{description}
\item  [Origins and Design Philosophy]
Ethernet evolved from a best-effort, general-purpose LAN protocol for office use. Reliability, ordering, and latency guarantees were added later (e.g., via TCP/IP, QoS, or RDMA over Converged Ethernet—RoCE).
InfiniBand was designed from the start for high-performance computing (HPC) and data centers, prioritizing low latency, high throughput, lossless transmission, and reliability at the transport layer.

\item [Transport and Reliability Model]
Ethernet (traditional) is best-effort. Packets may be dropped, duplicated, or arrive out of order. It's the responsibility of higher layers (e.g., TCP) to ensure reliability.
InfiniBand includes a reliable transport protocol in hardware, supporting retransmissions, acknowledgments, and flow control natively, without software stack involvement.

\item [Congestion and Flow Control]
Ethernet historically lacked built-in congestion control (early 802.3 Ethernet had none). Modern enhancements like Data Center Bridging (DCB) and Priority Flow Control (PFC) try to fix this, but they're complex and not universally deployed.
InfiniBand uses credit-based flow control and end-to-end congestion control, ensuring no buffer overruns and graceful behavior under load.

\item [Packet Loss Behavior]
Ethernet (especially under load or with improper configuration) will drop packets, which TCP must recover from. In large-scale systems, dropped packets can cause latency spikes or retries.
InfiniBand virtually never drops packets under normal operation. When congestion happens, packets are backpressured rather than discarded.

\item [Latency and Jitter]
Ethernet is not inherently low-latency. Latency and jitter vary depending on traffic conditions, switch buffering, and TCP behavior.
InfiniBand achieves microsecond-scale latencies with very low jitter, due to lightweight protocol stack, hardware offloads, and zero-copy RDMA support.

\item [RDMA Support]
Ethernet supports RDMA via RoCE, but it requires very careful network tuning (lossless fabric, PFC, ECN, etc.), which is brittle and often vendor-specific.
InfiniBand natively supports RDMA with hardware reliability, requiring no tuning or lossless Ethernet tricks.

\item [CPU Overhead]
Ethernet (via TCP/IP stack) incurs significant CPU overhead unless you’re using SmartNICs or offload engines.
InfiniBand offloads nearly everything—packetization, reliability, ordering—to the HCA (Host Channel Adapter), allowing much lower CPU utilization.

\item [Ecosystem and Deployment]
Ethernet is everywhere—ubiquitous, cheap, interoperable, and increasingly faster (100/200/400/800 Gbps).
InfiniBand is dominant in HPC and AI clusters where latency and throughput are critical, but it's niche, expensive, and vendor-constrained (mainly NVIDIA/Mellanox).
 
 \end{description}
 
 % WHERE IS THE QUERY HERE?
 
% Excellent. 
 
 Let’s start by exploring RoCE (RDMA over Converged Ethernet), then dig into why Ethernet dominates, despite its technical inferiority in some contexts.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{1. What is RoCE?}

RoCE stands for RDMA over Converged Ethernet, and it attempts to bring InfiniBand-style performance (especially RDMA) to the Ethernet world. It’s driven largely by:

The widespread dominance of Ethernet hardware.
The desire to reduce CPU overhead using zero-copy RDMA.
Avoiding the proprietary nature and cost of InfiniBand gear.
Variants:

RoCE v1: Works at Layer 2. Not routable—same Ethernet broadcast domain.

RoCE v2: Works at Layer 3. IP routable (UDP-based).

\subsection{2. Why RoCE is Hard to Deploy Reliably}

RoCE does not bring InfiniBand-level reliability automatically. Here's why:

\begin{itemize}
\item a. Requires Lossless Ethernet Fabric

RoCE depends on PFC (Priority Flow Control) to avoid packet loss, because it assumes a reliable transport.
But PFC is fragile and can cause deadlocks, head-of-line blocking, or cascading failures if misconfigured.
Engineers must carefully tune every switch for flow control, ECN, buffer sizing—an operational burden.

\item b. RoCE v2 is UDP-based
While it’s routable, it inherits none of the guarantees that InfiniBand hardware provides.
It relies on best-effort IP routing, unless the fabric is tightly managed.

\item c. RDMA Requires Ecosystem Support
You need NICs, drivers, libraries (e.g. libibverbs), and applications that support RDMA.
Without full-stack coordination, fallbacks to TCP/IP hurt performance.

\end{itemize}

\subsection{3. Why Ethernet Remains Dominant}

Despite these shortcomings, Ethernet continues to dominate outside of niche HPC environments. Here’s why:

\begin{itemize}
\item a. Ubiquity and Cost
Every datacenter already runs Ethernet. It’s cheap, commoditized, and standard.
Hardware (switches, NICs) is available from dozens of vendors.

\item b. Interoperability
Ethernet is vendor-agnostic. InfiniBand is dominated by NVIDIA/Mellanox, leading to vendor lock-in.

\item c. Evolving Performance
Ethernet speeds have scaled up dramatically: 10/25/50/100/200/400/800 Gbps.
With SmartNICs and programmable switches, it’s closing the gap.

\item d. Software Ecosystem
The entire Internet stack runs on Ethernet/TCP/IP. Switching is hard.
Tooling, monitoring, and network engineering knowledge is Ethernet-centric.

\item e. RoCE as a Bridge
While complex, RoCE offers a path to InfiniBand-like performance while preserving Ethernet investment.
Large-scale cloud providers (e.g., Azure) have successfully deployed RoCE with tight control over the fabric.
Conclusion:

\end{itemize}

InfiniBand is still the gold standard for ultra-low-latency, high-reliability HPC.

Ethernet, while historically less reliable, is catching up via:


RoCE (with caveats),

SmartNICs (e.g., BlueField, Pensando, etc.),

DCB and PFC tuning,

Fast software stacks (e.g., DPDK, eBPF).

Ethernet’s dominance is not because it’s better—it’s because it’s "good enough" and universal.

Would you like to dive deeper into SmartNICs or RDMA performance tuning? Or perhaps explore what an ideal next-gen fabric could look like, combining the best of both worlds?
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 
\subsection{Summary Table}
 
Feature	Ethernet (traditional)	InfiniBand

Reliability	Best-effort (via TCP)	Hardware-enforced

Latency	Millisecond-scale (typical)	Microsecond-scale

Jitter	High	Very low

Congestion Control	Optional (DCB/PFC)	Built-in

Packet Loss	Possible	Avoided by design

RDMA Support	RoCE (complex)	Native

CPU Overhead	High (software stack)	Low (hardware offload)

Use Case	General networking	HPC, AI, low-latency clusters

\end{document}
