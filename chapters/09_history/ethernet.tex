\section{Ethernet}
\marginnote{Sections taken from ./AE-Specifications-ETH/sections/BSW.tex}

Original 10~Mb/s Ethernet (and most `best-effort'' variants since) offers a
CRC to \emph{detect} corruption but no link-level
retransmission. Frames can be dropped by congestion, policing, or topology
loops. Hence raw Ethernet is both \emph{unreliable} \emph{and} \emph{fallible};
higher layers---typically TCP---supply ABP-like recovery.

\subsection{How InfiniBand Raises the Game}

InfiniBand \emph{embeds} ABP into silicon:

\begin{itemize}
\item Per-hop \emph{credit flow control} makes buffer overflow almost
      impossible.
\item Link-level CRC plus optional \emph{link retransmission} retries any
      corrupted frame.
\item Reliable Connection and Reliable Datagram queue pairs carry
      \emph{ACK/NACK} sequence numbers end-to-end, guaranteeing exactly-once,
      in-order delivery across multi-switch fabrics.
\end{itemize}

To software, the fabric appears nearly \emph{infallible}; drops are rare and localized.

\subsection{Reliable vs. Infallible, Unreliable vs, Fallible}

Table~\ref{tab:taxonomy} highlights the nuance.  Priority Flow Control (PFC)
can render an Ethernet link \emph{loss-less} in steady state, but deadlock,
mis-configuration, or burst congestion can still drop frames.  Such a link is
`reliable yet fallible." Infiniband's credit + retransmit pipeline, by contrast
shifts real-world operation toward `reliable and almost infallible.''

\subsection{Why Ethernet Still Struggles}

\begin{enumerate}
\item \textbf{Retrofitting}: inserting link retransmission into the IEEE\,802
      stack breaks long-standing timing and compatibility assumptions.
\item \textbf{Congestion domain}: shallow switch queues and ECMP paths leave
      more surfaces for loss than InfiniBand’s strict hop-by-hop credits.
\item \textbf{Layering philosophy}: because TCP `already'' ensures delivery,
      many operators accept occasional loss rather than pay silicon cost for
      hardware recovery.
\end{enumerate}

\subsection{Summary of Robert Garner’s Equations and Assumptions}
\marginnote{From ./AE-Specifications-ETH/standalone/Atomicity-ChatGPT-1.tex}

Below is a focused summary of Robert Garner’s key equations, assumptions, and reasoning, drawn from his two extensive email threads about ACK-based (reliable) protocols atop Ethernet-like links.

\subsection{Metcalfe’s Throughput Equation}

Garner references a classical result from Bob Metcalfe’s 1970s ARPANET-era work for stop-and-wait or limited-window protocols. In a modern notation, the \emph{effective capacity} $E$ of a channel is expressed as:
\[
  E \;=\; 
  \Bigl(\frac{S}{P}\Bigr)\;
  \times\;\underbrace{\frac{1}{1 + \frac{C\,T}{P}}}_{\text{Multiplexing factor } M}
  \;\times\;(1 - L)\;\times\;C,
\]
where:
\begin{itemize}
\item $C$ = channel capacity (bits/s),
\item $P$ = total packet length (bits),
\item $S$ = payload (data) bits per packet,
\item $L$ = probability of packet loss,
\item $T$ = transmitter timeout (round-trip delay + ACK time),
\end{itemize}
and the \emph{multiplexing factor} $M$ is
\[
  M \;=\;
  \frac{1}{1 + \frac{T}{T_p}}, 
  \quad \text{with} \quad 
  T_p \;=\;\frac{P}{C}.
\]
Typically, 
\[
T \;=\; 2d \;+\; \frac{A}{C},
\]
where 
\begin{itemize}
\item $d$ = one-way propagation delay,
\item $A$ = ACK packet size (bits).
\end{itemize}

\subsection{Example: Single Outstanding Packet on a 1.6\,m 100\,Gb/s Link}

In the emails, Garner applies these equations to a point-to-point link:
\begin{itemize}
\item Data packet size $P = 64\text{\,bytes} = 512\text{\,bits}$.
\item Link speed $C = 100\text{\,Gb/s}$.
\item One-way distance $1.6\text{\,m}$, propagation $\approx 3\text{\,ns/m}$, so $8\text{\,ns}$ round-trip.
\item ACK size $A = 8\text{\,bytes} = 64\text{\,bits}$.
\item Packet transmission time $T_p = P/C = 512 / (100\,\text{Gb/s}) = 5.12\,\text{ns}$.
\end{itemize}

Then
\[
  T = 2d + \frac{A}{C} = 8 + 0.64 = 8.64\,\text{ns}.
\]
Hence the multiplexing factor
\[
  M \;=\; \frac{1}{1 + \frac{8.64}{5.12}} \;\approx\; 0.37,
\]
meaning the link is at about $37\%$ of its raw capacity. If one adds a 2\,ns delay in the transmitter state machine to detect lost ACKs, $T$ becomes $10.64$\,ns, and
\[
  M \;=\; \frac{1}{1 + 10.64/5.12} \;\approx\; 0.32,
\]
so utilization drops to roughly one-third of link capacity.

\subsection{Multiple Outstanding Packets \& Reduced ACK Overhead}

Garner (and Bill Lynch) point out that \emph{supporting multiple packets in flight} or aggregated ACKs significantly increases throughput, since the size $P$ in the $M$ factor then reflects more bits in transit. If, for example, an ACK can cover 10 packets, the ratio $\frac{T}{T_p}$ shrinks, and the effective channel utilization can approach 80--90\% or more. Thus, \textbf{single-packet stop-and-wait} is a worst-case scenario for high-speed links.

\subsection{Additional Observations}

\begin{itemize}
\item \textbf{Loss Probability} $L$: Garner factors in $(1 - L)$ to capture the effect of rare losses that require retransmission.
\item \textbf{Real-World State Machine Delays}: Actual hardware detection and retransmission add extra time beyond the raw propagation and ACK bits, further reducing throughput.
\item \textbf{Layering vs.\ L2}: Garner questions whether ``perfect reliability'' at Layer 2 eliminates enough application-level failures to justify complexity. He notes that distributed systems still face node crashes, software bugs, and partial partitions.
\item \textbf{No Fundamental Limit}: ACK overhead by itself is not an insurmountable barrier to performance, provided the protocol allows multiple outstanding packets or otherwise reduces per-packet ACK latency.
\end{itemize}

\subsection{Key Takeaways}

\begin{enumerate}
\item \emph{Equation-Based Critique}: Metcalfe’s formula shows that single-outstanding-packet protocols can suffer large performance hits at high bit rates.
\item \emph{ACK Overhead} is not fatal if protocols support concurrency or aggregated ACKs, minimizing $\tfrac{T}{T_p}$.
\item \emph{Layer 2 vs.\ Layer 4}: Garner argues that application and node failures remain even if L2 is made ``perfectly reliable''. One still needs robust end-to-end protocols (e.g.\ at L4).
\item \emph{Practical Consequence}: Any new L2 reliability scheme must carefully manage concurrency, hardware delays, and aggregated ACK logic; otherwise throughput collapses (and tail latency may worsen).
\end{enumerate}


\newpage
\subsection{Response \#2}

%\documentclass[12pt]{article}
%\usepackage[margin=1in]{geometry}
%\usepackage{amsmath,amssymb}
%\begin{document}

\subsection*{A Change in Perspective}

Below is an illustrative reframing of the ``ACK-limited throughput'' problem by breaking a 64-byte packet into 8-byte slices and introducing ``sub-signals'' or ``sub-ACKs'' (\textbf{SACKs}). This perspective shows that once the protocol treats each slice as an ``unstoppable'' unit on the wire---and allows finer-grained acknowledgments---it mitigates the bandwidth throttling that arises from treating all 64\,bytes as a monolithic packet.

\subsection{From 64-Byte Packets to 8-Byte Slices}

\begin{itemize}
\item \textbf{Old Viewpoint:} A strict stop-and-wait protocol might force waiting for an ACK after sending each 64-byte chunk.  
At 100\,Gb/s, sending 64\,bytes (512 bits) takes about 5.12\,ns, but the round-trip delay plus ACK overhead could be 8--10\,ns or more, giving a poor utilization factor.

\item \textbf{New Viewpoint:} 
Break the 64\,bytes into eight 8-byte slices. Each slice (64 bits) is sent consecutively, so the wire is continuously stuffed with slices. Sub-ACKs (SACKs) arrive on a finer timescale, enabling the sender to pipeline slices without fully stalling for a 64-byte ACK.
\end{itemize}

\subsection{Time Calculations with Slices}

\begin{itemize}
\item \textbf{Link capacity}: $C = 100$\,Gb/s.
\item \textbf{One 8-byte slice} = 8~$\times$~8\,bits = 64\,bits.
\item \textbf{Transmit time per slice}:
\[
  T_{\text{slice}} \;=\; \frac{64 \,\text{bits}}{100\,\text{Gb/s}}
  \;=\; 0.64\,\text{ns}.
\]
\item A 64-byte packet is thus 8 consecutive slices, so $8 \times 0.64\,\text{ns} = 5.12\,\text{ns}$ total transmission. 
\end{itemize}

Because sub-ACKs can confirm earlier slices, the transmitter no longer needs to wait for a single bulk ACK after the entire 64\,bytes are sent. This partial or finer-grained acknowledgment fosters multiple in-flight slices.

\subsection{Reduced Throttling via Structured Stop-and-Wait}

\begin{itemize}
\item \textbf{Original Single-Packet Stall:}\\
If the protocol waits for a 64-byte ACK after sending each 64\,bytes, a large fraction of time is idle (the round-trip plus ACK overhead).

\item \textbf{Structured SACK:}\\
Treat each 8-byte slice as unstoppable on the wire, sending further slices as earlier ones are sub-ACKed.  By the time the last slices of a 64-byte chunk are on the wire, the first slices are already acknowledged. This pipelines transmissions and reduces idle periods, thus boosting effective bandwidth.
\end{itemize}

\subsection{Intuitive Bandwidth Gain}

By subdividing packets into 8-byte slices, we reduce the ratio of ``waiting time'' to ``sending time.'' Over an 8--10\,ns round-trip, multiple 0.64\,ns slices fit in flight.  The pipeline sees a larger effective window, maintaining higher link utilization.  Mathematically, in the standard throughput formula 
\[
M \;=\; \frac{1}{1 + \frac{T}{T_p}},
\]
the in-flight slices raise the denominator's $T_p$ (total bits per pipeline) and thus increase $M$ significantly compared to a single 64-byte chunk with a single stop-and-wait phase.

\subsection{Conclusion}

By \textbf{changing our perspective} from a monolithic 64-byte packet to \textbf{eight unstoppable 8-byte slices}, each with sub-ACK signals (SACKs), we effectively pipeline the stop-and-wait protocol. This granular approach keeps multiple slices in flight during any round-trip interval, eliminating most idle-wire time and mitigating the throughput loss. Hence, subdividing 64-byte packets into smaller slices with finer-grained acknowledgments allows higher channel utilization and reduces stop-and-wait throttling.
