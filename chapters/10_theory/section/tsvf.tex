\documentclass[../../../OAE-SPEC-MAIN.tex]{subfiles}
\begin{document}

\section{Two-State Vector Formalism}

The \emph{Two-State Vector Formalism} (TSVF), developed by Aharonov and collaborators, describes a quantum system using both a forward-evolving state vector from the past and a backward-evolving vector from the future, forming a complete description of the system between two time boundaries.

This formalism maps intriguingly well onto the two-phase behavior of supervised learning:

\begin{itemize}
  \item \textbf{Forward propagation:} evolving the input forward through the network to predict an output.
  \item \textbf{Backpropagation:} retroactively applying a loss function at the output and propagating error information backward through the network to adjust weights.
\end{itemize}

\begin{highlightbox}
Forward propagation plays the role of the forward-evolving quantum state \(|\psi(t)\rangle\), and backpropagation corresponds to the backward-evolving dual state \(\langle \phi(t)|\). Together, they constrain the learning dynamics at each layer via a two-state viewpoint.
\end{highlightbox}

\subsection{Forward Propagation as Forward Evolution}

In TSVF:
\[
|\psi(t)\rangle = U(t, t_0) |\psi(t_0)\rangle,
\]
where \(U\) is the unitary time evolution operator from an initial preparation at \(t_0\).

In machine learning:
\[
a^{(l+1)} = f^{(l)}(W^{(l)} a^{(l)} + b^{(l)}),
\]
where the activations \(a^{(l)}\) propagate the input forward.

\subsection{Backpropagation as Backward Evolution}

In TSVF, a post-selected state \(\langle\phi(t_1)|\) evolves backward:
\[
\langle \phi(t)| = \langle \phi(t_1)| U(t_1, t),
\]
complementing the forward-evolving \(|\psi(t)\rangle\).

In neural nets:
\[
\delta^{(l)} = (W^{(l+1)})^\top \delta^{(l+1)} \circ f'^{(l)}(z^{(l)}),
\]
where \(\delta^{(l)}\) encodes error at layer \(l\) and propagates backward to adjust weights.

\subsection{Two-State Update Rule}

In TSVF, expectation values take the form:
\[
\langle A \rangle_w = \frac{\langle \phi | A | \psi \rangle}{\langle \phi | \psi \rangle},
\]
and represent weak values or amplitudes constrained by both past and future states.

In machine learning, the update rule:
\[
\Delta W^{(l)} \propto \delta^{(l)} (a^{(l-1)})^\top
\]
depends on the forward activations \(a^{(l-1)}\) and backward error signal \(\delta^{(l)}\). Together, they act like a sandwich operator:
\[
\text{Update}^{(l)} \sim \langle \phi^{(l)} | \text{operator} | \psi^{(l)} \rangle.
\]

\subsection{Time-Symmetric Learning View}

Rather than treating backpropagation as a mere computational trick, TSVF offers a time-symmetric interpretation:

\begin{itemize}
  \item Both the input and the desired output state determine the intermediate learning dynamics.
  \item Each layer mediates between past input and future supervision, forming a time-bridging node.
\end{itemize}

\begin{table}[htbp]
  \centering
  \footnotesize
  \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{Concept} & \textbf{Neural Network} & \textbf{TSVF QM} \\
    \midrule
    Initial input        & \(x\)                       & \(|\psi(t_0)\rangle\) \\
    Prediction process   & Forward prop                & \(U(t, t_0)\) evolution \\
    Target supervision   & Loss function               & Post-selection \(\langle\phi(t_1)|\) \\
    Error signal         & \(\delta^{(l)}\)            & \(\langle\phi(t)|\) \\
    Intermediate activity& \(a^{(l)}\)                 & \(|\psi(t)\rangle\) \\
    Weight update        & \(\delta^{(l)} (a^{(l-1)})^\top\) & \(\langle \phi | A | \psi \rangle\) \\
    \bottomrule
  \end{tabular}
  \caption{Analogies between supervised learning and the Two-State Vector Formalism (TSVF) in quantum mechanics.}
  \label{tab:tsvf_analogy}
\end{table}

\subsection{FITO Thinking vs. Time Symmetry}

Most machine learning frameworks assume \emph{Forward-In-Time-Only (FITO)} causality: input causes output, and learning proceeds only by adjusting from past to future. TSVF suggests a richer model:

\begin{itemize}
  \item Supervision from the future constrains the learning of the past.
  \item This bidirectional model aligns with concepts from goal-driven behavior and active inference.
\end{itemize}

\subsection{Conclusion}

The TSVF reframing of forward and backpropagation illuminates the deeper time-symmetric structure underlying learning. Far from being just a computational trick, backpropagation can be seen as a physical dual to forward propagationâ€”both necessary to fully specify a learning system between two boundary conditions.

 
This is highly relevant to our model of the protocol, where we use reversible state machines to specify forward propagation, with whatever reversible glitches might be needed to handle failures are implemented as reversible steps, and the backpropagation  as the credit-based flow control mechanism.

Because they are completely symmetric, packets being sent and packets being unsent are fully managed by the flow control system.

\end{document}
